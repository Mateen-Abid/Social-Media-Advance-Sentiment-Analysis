# -*- coding: utf-8 -*-
"""sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-S1ZZXF1VbHWfqNbURTni-t6sS9Xut_t
"""

!pip install transformers datasets --upgrade

import kagglehub

# Download latest version
path = kagglehub.dataset_download("debarshichanda/goemotions")

print("Path to dataset files:", path)

"""# **Load and Explore the GoEmotions Dataset**"""

from datasets import load_dataset

# Load the GoEmotions dataset
dataset = load_dataset("go_emotions")
print("Dataset Splits:", dataset)

# Print a sample example
print("\nSample Example:\n", dataset["train"][0])

"""# **Preprocess the Data (convert multi-label to single label)**"""

# Select only the first label from the list and remove the 'labels' field
def preprocess_function(example):
    example["label"] = example["labels"][0] if len(example["labels"]) > 0 else 0
    del example["labels"]
    return example

dataset = dataset.map(preprocess_function)
print("\nPreprocessed Sample:\n", dataset["train"][0])

"""# **Tokenize the Text**"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Format the tokenized dataset for PyTorch
tokenized_datasets.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

print("\nTokenized Sample:\n", tokenized_datasets["train"][0])

"""# **Load BERT Model for Sequence Classification**"""

from transformers import AutoModelForSequenceClassification

# 28 classes for emotions
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=28)
print("\nModel Loaded: BERT-base with classification head for 28 labels")

"""# **Set Training Arguments & Trainer**"""

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=2,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
)

"""# **Train the Model**"""

print("Starting Training...")
trainer.train()
print("Training Completed")

"""# **Evaluate the Model**"""

# Evaluate on validation set
metrics = trainer.evaluate()
print("\n Evaluation Metrics:\n", metrics)

"""# **Save the Fine-Tuned Model**"""

# Save model and tokenizer
model.save_pretrained("./fine_tuned_bert_goemotions")
tokenizer.save_pretrained("./fine_tuned_bert_goemotions")

print(" Model and tokenizer saved successfully!")

"""# **Predict Emotion on New Text (Custom Input)**"""

import torch
import numpy as np

emotions = [
    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',
    'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',
    'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',
    'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'
]

def predict_emotion(text):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    model.to(device)

    # Tokenize and move inputs to device
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=1)

    # Get top prediction and confidence
    predicted_idx = torch.argmax(probs, dim=1).item()
    predicted_label = emotions[predicted_idx]
    confidence = probs[0, predicted_idx].item()

    return predicted_label, round(confidence, 3)

# Test the function
text = "I’m wondering who will be the president next year"
emotion, confidence = predict_emotion(text)
print(f"\nText: \"{text}\"\nPredicted Emotion: {emotion} ({confidence * 100:.1f}%)")

"""# **Visualization**

**TOP 5 Predicted Emotions**
"""

pip install matplotlib seaborn

import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_bert_goemotions")
model = AutoModelForSequenceClassification.from_pretrained("./fine_tuned_bert_goemotions")

emotions = [
    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',
    'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',
    'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',
    'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'
]

def plot_emotion_probs(probs_tensor, emotions):
    probs = probs_tensor[0].cpu().numpy()
    top_indices = np.argsort(probs)[::-1][:5]  # Top 5 emotions

    top_emotions = [emotions[i] for i in top_indices]
    top_values = [probs[i] for i in top_indices]

    sns.set(style="whitegrid")
    plt.figure(figsize=(8, 5))
    sns.barplot(x=top_values, y=top_emotions, palette="viridis")
    plt.title("Top 5 Predicted Emotions")
    plt.xlabel("Probability")
    plt.xlim(0, 1)
    plt.tight_layout()
    plt.show()
text = "I’m wondering who will be the president next year"

# Tokenize the text
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)

with torch.no_grad():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = inputs.to(device)
    model.to(device)
    # Make predictions
    outputs = model(**inputs)
    logits = outputs.logits
    probs = torch.nn.functional.softmax(logits, dim=1)

# Plot the probabilities
plot_emotion_probs(probs, emotions)